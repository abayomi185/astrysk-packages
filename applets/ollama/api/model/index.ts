/**
 * Generated by orval v7.2.0 üç∫
 * Do not edit manually.
 * Ollama REST API
 * [Ollama](https://ollama.ai/) allows you to run powerful LLM models locally on your machine, and exposes a REST API to interact with them on localhost.

Based on the [official Ollama API docs](https://github.com/jmorganca/ollama/blob/main/docs/api.md)

### Getting started

1. Download [Ollama](https://ollama.ai/)
    
2. Pull a model, following [instructions](https://github.com/jmorganca/ollama)
    
3. Fire up localhost with `ollama serve`
 * OpenAPI spec version: 1.0.0
 */

export * from "./chatBody";
export * from "./create200";
export * from "./createBody";
export * from "./deleteAModelBody";
export * from "./generate200";
export * from "./generate200AnyOf";
export * from "./generate200AnyOfTwo";
export * from "./generateBody";
export * from "./generateEmbedding200";
export * from "./generateEmbeddingBody";
export * from "./listLocalModels200";
export * from "./listLocalModels200ModelsItem";
export * from "./pullAModelBody";
export * from "./pushAModelBody";
export * from "./showModel200";
export * from "./showModelBody";
